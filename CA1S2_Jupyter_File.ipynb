{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa906bd1",
   "metadata": {},
   "source": [
    "### PySpark reading a file form a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38537e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b455da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:32:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigData\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a6244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dffaa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "df_emails.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda0b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|summary|                file| message|\n",
      "+-------+--------------------+--------+\n",
      "|  count|             8299853| 2508249|\n",
      "|   mean|                 NaN|Infinity|\n",
      "| stddev|                 NaN|     NaN|\n",
      "|    min|                  \\t|      \\t|\n",
      "|    max|~~~~~~~~~~~~~~~~~...|       ||\n",
      "+-------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe provides summary statistics of numeric columns in a DataFrame\n",
    "df_emails.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3e61cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows\n",
    "df_emails.show(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a150d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|ExtractedDate|\n",
      "+-------------+\n",
      "|             |\n",
      "|             |\n",
      "|null         |\n",
      "|null         |\n",
      "|null         |\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Example regular expression pattern for a date in the format \"E, dd MMM yyyy HH:mm:ss Z\"\n",
    "# Adjust this pattern to match the actual format found in your 'message' data\n",
    "date_pattern = r'\\bMon, \\d{2} \\w{3} \\d{4} \\d{2}:\\d{2}:\\d{2} -\\d{4} \\(PDT\\)'\n",
    "\n",
    "# Create a new column 'ExtractedDate' by extracting the date string from 'message'\n",
    "df_emails = df_emails.withColumn(\"ExtractedDate\", regexp_extract(\"message\", date_pattern, 0))\n",
    "\n",
    "# Show the result of extraction\n",
    "df_emails.select(\"ExtractedDate\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75883be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c7f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|message_clean               |\n",
      "+----------------------------+\n",
      "|messageid javamailevansthyme|\n",
      "| may pdt                    |\n",
      "|null                        |\n",
      "|null                        |\n",
      "|null                        |\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Assuming SparkSession has already been created\n",
    "# spark = SparkSession.builder.appName(\"EmailsAnalysis\").getOrCreate()\n",
    "\n",
    "# Make sure df_emails is properly defined here\n",
    "# df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "\n",
    "# Convert the text to lower case\n",
    "df_emails = df_emails.withColumn(\"message_clean\", lower(col(\"message\")))\n",
    "\n",
    "# Remove email metadata, non-letter characters, and extra spaces\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"^(From:|To:|Subject:|Mime-Version:|Content-Type:|Content-Transfer-Encoding:|X-From:|X-To:|X-cc:|X-bcc:|X-Folder:|X-Origin:|X-FileName:).*\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"\\s+\", \" \"))\n",
    "\n",
    "# Show the cleaned text\n",
    "df_emails.select(\"message_clean\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bdb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:33:32 WARN DAGScheduler: Broadcasting large task binary with size 1803.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[0,70,202,263,330,474,578,646,680,728,831,886,1184,1250,1264,1369,2071,2266,2742,2840,7311,8723,10224,15635,40555],[0.1015382270840639,6.019066583853422,6.743844934454249,6.919012653891585,7.078540971674206,7.323663429707191,7.479953147783199,7.564825486524079,7.596398748440169,7.67824741627145,7.7866102101032455,7.839385024085388,8.11824450792199,8.175243951886396,8.183864694930303,8.30816241160788,8.67280552519579,8.740958413433015,8.956573698326434,9.004582917512794,10.16054650265237,10.420829600916036,10.647357250214487,11.259158791320479,12.64545315244037])|\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[0,2844],[0.1015382270840639,9.004582917512794])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|(115235,[0,17,5426],[0.1015382270840639,5.08948268439694,9.769067636518946])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:34:02 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, to_timestamp\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ensure df_emails is correctly defined and available at this point in your code\n",
    "\n",
    "# Filter out rows where 'message_clean' is null or an empty string\n",
    "df_emails_filtered = df_emails.filter(col(\"message_clean\").isNotNull() & (col(\"message_clean\") != \"\"))\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"message_clean\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
    "\n",
    "# Apply the pipeline to the filtered DataFrame\n",
    "model = pipeline.fit(df_emails_filtered)\n",
    "result = model.transform(df_emails_filtered)\n",
    "\n",
    "# Show the result\n",
    "result.select(\"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271cb90",
   "metadata": {},
   "source": [
    "##### Your model successfully transformed the textual data into numerical vectors that can be used for machine learning purposes, including neural network models for detecting suspicious messages. The warning about broadcasting a large task binary size is an indication of the data size being processed but is generally not a concern unless it leads to performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee82cb",
   "metadata": {},
   "source": [
    "## Step 4: Designing the Neural Network which come  Before moving on to training a neural network model, we'll need to prepare your dataset further, including splitting it into training and test sets, and potentially normalizing the feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42b57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6889c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:34:04 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:35:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:12 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:13 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:13 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:15 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:15 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:17 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:17 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/21 07:36:18 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.9982875492536734\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Example UDF to label emails based on the presence of a \"suspicious keyword\"\n",
    "def label_email(content):\n",
    "    if content is not None and \"cash\" in content:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Register the UDF\n",
    "label_udf = udf(label_email, IntegerType())\n",
    "\n",
    "# Assuming 'result' is your DataFrame and 'message_clean' is the column containing the cleaned email text\n",
    "# Apply the UDF to create a new column 'label'\n",
    "result = result.withColumn('label', label_udf(col('message_clean')))\n",
    "\n",
    "# Now proceed with data preparation steps such as splitting the dataset into training and test sets\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Ensure your model training code below is correctly referring to 'features' and 'label' columns\n",
    "# For example:\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the classifier, assuming 'features' column contains vectorized features\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lrModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model if necessary\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726f7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 07:37:20.166483: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-21 07:37:20.369991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-21 07:37:20.370086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-21 07:37:20.405824: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-21 07:37:20.487106: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-21 07:37:20.488693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 07:37:21.764078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 3s 182ms/step - loss: 0.6930 - accuracy: 0.5175 - val_loss: 0.6925 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.6736 - accuracy: 0.7337 - val_loss: 0.6925 - val_accuracy: 0.5050\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 1s 107ms/step - loss: 0.5509 - accuracy: 0.9388 - val_loss: 0.6973 - val_accuracy: 0.4900\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 1s 113ms/step - loss: 0.2563 - accuracy: 0.9800 - val_loss: 0.7784 - val_accuracy: 0.5600\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.0632 - accuracy: 0.9987 - val_loss: 1.0506 - val_accuracy: 0.5600\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.0274 - accuracy: 0.9987 - val_loss: 1.1241 - val_accuracy: 0.5350\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.0112 - accuracy: 0.9987 - val_loss: 1.6163 - val_accuracy: 0.4700\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.0268 - val_accuracy: 0.4450\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.1860 - val_accuracy: 0.4400\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.3272 - val_accuracy: 0.4400\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.5335 - val_accuracy: 0.4450\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.7899 - val_accuracy: 0.4350\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.8731 - val_accuracy: 0.4350\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.9578 - val_accuracy: 0.4400\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 1s 114ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.0339 - val_accuracy: 0.4400\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.1539 - val_accuracy: 0.4450\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.2701 - val_accuracy: 0.4400\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.3350 - val_accuracy: 0.4400\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 9.6833e-04 - accuracy: 1.0000 - val_loss: 3.4426 - val_accuracy: 0.4300\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 9.0152e-04 - accuracy: 1.0000 - val_loss: 3.5236 - val_accuracy: 0.4300\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 9.0492e-04 - accuracy: 1.0000 - val_loss: 3.5704 - val_accuracy: 0.4350\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 8.1840e-04 - accuracy: 1.0000 - val_loss: 3.5981 - val_accuracy: 0.4350\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 8.1247e-04 - accuracy: 1.0000 - val_loss: 3.6626 - val_accuracy: 0.4400\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 7.3719e-04 - accuracy: 1.0000 - val_loss: 3.7230 - val_accuracy: 0.4350\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 6.6438e-04 - accuracy: 1.0000 - val_loss: 3.7767 - val_accuracy: 0.4350\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5.4812e-04 - accuracy: 1.0000 - val_loss: 3.8373 - val_accuracy: 0.4300\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5.4964e-04 - accuracy: 1.0000 - val_loss: 3.8651 - val_accuracy: 0.4400\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5.4700e-04 - accuracy: 1.0000 - val_loss: 3.9151 - val_accuracy: 0.4300\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5.0911e-04 - accuracy: 1.0000 - val_loss: 3.9670 - val_accuracy: 0.4350\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 4.8890e-04 - accuracy: 1.0000 - val_loss: 4.0197 - val_accuracy: 0.4350\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 4.5230e-04 - accuracy: 1.0000 - val_loss: 4.0331 - val_accuracy: 0.4450\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 4.2869e-04 - accuracy: 1.0000 - val_loss: 4.0575 - val_accuracy: 0.4500\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 6s 485ms/step - loss: 4.3352e-04 - accuracy: 1.0000 - val_loss: 4.0911 - val_accuracy: 0.4500\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 20s 1s/step - loss: 4.5625e-04 - accuracy: 1.0000 - val_loss: 4.1491 - val_accuracy: 0.4450\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 10s 710ms/step - loss: 4.0846e-04 - accuracy: 1.0000 - val_loss: 4.1738 - val_accuracy: 0.4500\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 2s 147ms/step - loss: 3.7846e-04 - accuracy: 1.0000 - val_loss: 4.1617 - val_accuracy: 0.4450\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 3s 197ms/step - loss: 3.3325e-04 - accuracy: 1.0000 - val_loss: 4.1932 - val_accuracy: 0.4450\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 2s 139ms/step - loss: 3.3065e-04 - accuracy: 1.0000 - val_loss: 4.2585 - val_accuracy: 0.4500\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 3.1143e-04 - accuracy: 1.0000 - val_loss: 4.2938 - val_accuracy: 0.4500\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 1s 108ms/step - loss: 3.0634e-04 - accuracy: 1.0000 - val_loss: 4.3331 - val_accuracy: 0.4500\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 2s 124ms/step - loss: 3.0069e-04 - accuracy: 1.0000 - val_loss: 4.3524 - val_accuracy: 0.4550\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 2s 131ms/step - loss: 2.7299e-04 - accuracy: 1.0000 - val_loss: 4.3849 - val_accuracy: 0.4550\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 2s 133ms/step - loss: 3.0934e-04 - accuracy: 1.0000 - val_loss: 4.4191 - val_accuracy: 0.4600\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 2.7556e-04 - accuracy: 1.0000 - val_loss: 4.4608 - val_accuracy: 0.4550\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 2.5901e-04 - accuracy: 1.0000 - val_loss: 4.5134 - val_accuracy: 0.4500\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 2.6930e-04 - accuracy: 1.0000 - val_loss: 4.5476 - val_accuracy: 0.4500\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 2.5364e-04 - accuracy: 1.0000 - val_loss: 4.5805 - val_accuracy: 0.4500\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 2s 113ms/step - loss: 2.2598e-04 - accuracy: 1.0000 - val_loss: 4.5755 - val_accuracy: 0.4550\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 1.9042e-04 - accuracy: 1.0000 - val_loss: 4.5948 - val_accuracy: 0.4550\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 2.1027e-04 - accuracy: 1.0000 - val_loss: 4.6461 - val_accuracy: 0.4500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 3.6966 - accuracy: 0.5350\n",
      "Test Accuracy: 0.5350000262260437\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "# For example, let's assume you've loaded and prepared your datasets into these variables\n",
    "# X_train, X_test, y_train, y_test = load_and_preprocess_your_data()\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000,  # Size of your vocabulary\n",
    "              output_dim=128,  # Dimension of the dense embedding\n",
    "              input_length=100),  # Length of input sequences\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c89901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 7s 268ms/step - loss: 0.6934 - accuracy: 0.5188 - val_loss: 0.6953 - val_accuracy: 0.4600\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 0.6929 - accuracy: 0.5175 - val_loss: 0.6939 - val_accuracy: 0.4600\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 0.6920 - accuracy: 0.5175 - val_loss: 0.6937 - val_accuracy: 0.4600\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 0.6911 - accuracy: 0.5450 - val_loss: 0.6978 - val_accuracy: 0.4600\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 3s 239ms/step - loss: 0.6882 - accuracy: 0.5250 - val_loss: 0.7000 - val_accuracy: 0.4600\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.7000 - accuracy: 0.4700\n",
      "Test Accuracy: 0.4699999988079071\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),  # Increased complexity and added dropout\n",
    "    LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Using RMSprop optimizer and setting a learning rate\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34a811d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5067525e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8f43be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14435/2882071045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Initialize and fit the tokenizer on the 'message_clean' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_emails_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Convert texts to sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    294\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 08:17:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "24/03/21 08:17:18 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.2.15:39701 in 10000 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Fill None values with an empty string in the 'message_clean' column\n",
    "df_emails_pd['message_clean'] = df_emails_pd['message_clean'].fillna('')\n",
    "\n",
    "# Now proceed with fitting the tokenizer\n",
    "tokenizer.fit_on_texts(df_emails_pd['message_clean'])\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "\n",
    "# Assuming 'message' is the column with email content that you want to preprocess and predict\n",
    "df_emails = df_emails.withColumnRenamed('message', 'message_clean')  # Only if renaming is necessary\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for processing with Keras\n",
    "df_emails_pd = df_emails.toPandas()\n",
    "\n",
    "# Initialize and fit the tokenizer on the 'message_clean' column\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_emails_pd['message_clean'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df_emails_pd['message_clean'])\n",
    "\n",
    "# Pad sequences\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Add predictions back to the Pandas DataFrame (as a new column)\n",
    "df_emails_pd['fraud_probability'] = predictions\n",
    "\n",
    "# If you want to work with Spark DataFrame afterward, convert it back\n",
    "df_emails_updated = spark.createDataFrame(df_emails_pd)\n",
    "\n",
    "# Show some predictions\n",
    "df_emails_updated.select('message_clean', 'fraud_probability').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adf0e3f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `message_clean` cannot be resolved. Did you mean one of the following? [`file`, `message`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14435/3672819675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Initialize and fit the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_emails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Convert texts to sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \"\"\"\n\u001b[1;32m   2927\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2928\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2929\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2930\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `message_clean` cannot be resolved. Did you mean one of the following? [`file`, `message`]."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# Load your saved model\n",
    "model = load_model('path_to_my_model.h5')\n",
    "\n",
    "# Assuming df_emails is a Pandas DataFrame and has been preprocessed\n",
    "# If df_emails is a Spark DataFrame, you'll need to convert it to Pandas DataFrame\n",
    "# df_emails = spark_df.toPandas()  # Only if starting with a Spark DataFrame\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "# These should match the parameters used during training\n",
    "max_words = 10000  # This should match the tokenizer setting during training\n",
    "maxlen = 100  # This should match the sequence padding length during training\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_emails['message_clean'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df_emails['message_clean'])\n",
    "\n",
    "# Pad sequences\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Add predictions back to the DataFrame (as a new column)\n",
    "df_emails['fraud_probability'] = predictions\n",
    "\n",
    "# Visualize some of the predictions\n",
    "print(df_emails[['message_clean', 'fraud_probability']].head())\n",
    "\n",
    "# If you need to convert back to Spark DataFrame\n",
    "# df_emails_spark = spark.createDataFrame(df_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e439d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fba37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478039e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287c937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d7995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02add97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Data Frame Linux Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947087a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the CSV into 4 parts with a prefix 'data_chunk_'\n",
    "# The -n l/4 option splits the file into 4 equal parts, by lines\n",
    "#This will create files named data_chunk_00, data_chunk_01,\n",
    "\n",
    "###   split -n l/10 -d your_large_file.csv data_chunk_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953c2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:40:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmailClassification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Replace the path below with the path to your Hadoop-stored file\n",
    "df_emails = spark.read.csv(\"hdfs:///user1/data_batch_00\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed2126dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "# Assuming the text column is named 'message'\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_emails)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Continue with further processing as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da825b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:40:14 ERROR Executor: Exception in task 1.0 in stage 41.0 (TID 111)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n",
      "\t... 18 more\n",
      "24/03/21 07:40:14 ERROR Executor: Exception in task 0.0 in stage 41.0 (TID 110)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n",
      "\t... 18 more\n",
      "24/03/21 07:40:14 WARN TaskSetManager: Lost task 1.0 in stage 41.0 (TID 111) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n",
      "\t... 18 more\n",
      "\n",
      "24/03/21 07:40:14 ERROR TaskSetManager: Task 1 in stage 41.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o533.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 41.0 failed 1 times, most recent failure: Lost task 1.0 in stage 41.0 (TID 111) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14435/1272679267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# If everything is correct up to here, then converting to a Pandas DataFrame should work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpandas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o533.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 41.0 failed 1 times, most recent failure: Lost task 1.0 in stage 41.0 (TID 111) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4493/510165404: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Assuming 'df_emails' is your initial DataFrame and 'message' is the column with email texts\n",
    "# Ensure 'df_emails' has been defined correctly and contains the 'message' column\n",
    "\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_emails)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Check if the 'filtered_words' column exists\n",
    "filtered_df.printSchema()\n",
    "\n",
    "# If everything is correct up to here, then converting to a Pandas DataFrame should work\n",
    "pandas_df = filtered_df.select(\"filtered_words\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Filter out rows where the message column is null\n",
    "df_non_null = df_emails.filter(col(\"message\").isNotNull())\n",
    "\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_non_null)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Continue with your processing...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "pandas_df = filtered_df.select(\"filtered_words\").toPandas()\n",
    "\n",
    "# Now, use Keras' text processing tools as shown previously to tokenize and pad sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de264bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Placeholder for your data\n",
    "texts = [\"Your preprocessed email texts here...\"]  # This should be a list of email texts\n",
    "labels = [0, 1, 0, 1]  # Binary labels for each text\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)  # num_words is the size of your vocabulary\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)  # maxlen is the length of the longest sequence\n",
    "\n",
    "# Assuming labels are already prepared\n",
    "import numpy as np\n",
    "labels = np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e201322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example placeholder, replace these with actual preprocessed data and labels\n",
    "texts = [\"sample text 1\", \"sample text 2\", \"sample text 3\", \"sample text 4\"]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)  # Ensure this matches your sequence length\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Now proceed with train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=16, input_length=100),  # input_dim is the size of the vocabulary, output_dim is the dimension of the dense embedding\n",
    "    GlobalAveragePooling1D(),  # This will average the embeddings of all words in the sequence\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification (0 or 1 labels)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786dc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=100, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd65014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb1417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacf2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa222a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdea37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f5cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8aed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f74a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786ff41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdc174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bce61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdda03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a53c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffcdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795585fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad373248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3c67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9493ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37106002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abbcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59fc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54143c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9812fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd12ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dc0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f1b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004be08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50146c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b25a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08912385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ff6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9930f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b26313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f58620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8629c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75c970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17e8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66ebcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c3109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b61bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee443af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32528d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26331c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96545a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5949c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0dfa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa3927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced810d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a91b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd28b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab87719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a04e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b4e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b2cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02541bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2b0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16c620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedf085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca78655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e3c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d886b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c47354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f3743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f669ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac73272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bf171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe21f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fraud_predictions(email_texts, predictions):\n",
    "    \"\"\"\n",
    "    Visualize the emails with their predicted fraud probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - email_texts: List of email text content.\n",
    "    - predictions: List of predicted probabilities corresponding to the fraud likelihood of each email.\n",
    "\n",
    "    The function doesn't return anything but prints each email with its fraud prediction.\n",
    "    \"\"\"\n",
    "    # Ensure the lists have the same length\n",
    "    if len(email_texts) != len(predictions):\n",
    "        print(\"Error: The length of email_texts and predictions must match.\")\n",
    "        return\n",
    "    \n",
    "    for email, probability in zip(email_texts, predictions):\n",
    "        print(\"Email Content:\\n\", email)\n",
    "        # Assuming the probability is given in a scale from 0 to 1\n",
    "        print(\"Fraud Likelihood: {:.2%}\".format(probability))\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# Assuming the predictions list matches the number of email_texts provided\n",
    "predicted_probabilities = [0.34725702, 0.08264993]\n",
    "\n",
    "# Example usage with matching lengths for email_texts and predicted_probabilities\n",
    "visualize_fraud_predictions(email_texts, predicted_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a3b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f68a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emails_by_similarity_and_likelihood(emails, similarity_threshold=0.5, likelihood_threshold=8.0):\n",
    "    \"\"\"\n",
    "    Filters emails based on content similarity to a given phrase and a likelihood threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - emails: List of dictionaries, where each dictionary contains 'content' and 'likelihood' keys.\n",
    "    - similarity_threshold: A threshold for determining content similarity (not used in this simple example).\n",
    "    - likelihood_threshold: The minimum likelihood score for an email to be considered suspicious.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of emails considered suspicious based on the likelihood threshold.\n",
    "    \"\"\"\n",
    "    suspicious_phrase = \"Another suspicious email detected!\"\n",
    "    filtered_emails = [email for email in emails if suspicious_phrase in email['content'] and email['likelihood'] >= likelihood_threshold]\n",
    "    return filtered_emails\n",
    "\n",
    "# Example usage:\n",
    "emails = [\n",
    "    {'content': \"This is a normal email content.\", 'likelihood': 2.0},\n",
    "    {'content': \"Another suspicious email detected! Please check it out.\", 'likelihood': 8.26},\n",
    "    {'content': \"Another suspicious email detected! This seems like a scam.\", 'likelihood': 9.5},\n",
    "    {'content': \"This is another normal conversation.\", 'likelihood': 3.2}\n",
    "]\n",
    "\n",
    "# Filtering emails:\n",
    "suspicious_emails = filter_emails_by_similarity_and_likelihood(emails, likelihood_threshold=8.0)\n",
    "\n",
    "# Displaying the filtered, suspicious emails:\n",
    "for email in suspicious_emails:\n",
    "    print(f\"Email Content: {email['content']}\")\n",
    "    print(f\"Fraud Likelihood: {email['likelihood']}%\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73ec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd9ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f113b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa12248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74514c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a04849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f426539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc31de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26079f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5844a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde65d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169719d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85238ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df98e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
