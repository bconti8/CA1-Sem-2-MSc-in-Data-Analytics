{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa906bd1",
   "metadata": {},
   "source": [
    "### PySpark reading a file form a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38537e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a356b361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 09:20:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EmailSuspicionDetection\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f409c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming the file is in CSV format and has a header\n",
    "df = spark.read.csv(\"hdfs:///user1/data_batch_00\", header=True, inferSchema=True)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fee81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67718b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c7484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e090cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67867a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61facdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ff2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855b60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebc2425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EmailSuspicionDetection\").getOrCreate()\n",
    "\n",
    "# Assuming the file is in CSV format and has a header\n",
    "df = spark.read.csv(\"hdfs:///user1/data_batch_00\", header=True, inferSchema=True)\n",
    "\n",
    "# Filtering out rows where the message column is null\n",
    "df = df.filter(col(\"message\").isNotNull())\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "df_tokenized = tokenizer.transform(df)\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df_cleaned = remover.transform(df_tokenized)\n",
    "\n",
    "# Hashing term frequency\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(df_cleaned)\n",
    "\n",
    "# Inverse document frequency\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a618cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert the specific column of the DataFrame to a list\n",
    "email_texts = df.select(\"message\").rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d2d9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)  # This will keep the top 10,000 most frequent words\n",
    "tokenizer.fit_on_texts(email_texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(email_texts)\n",
    "X = pad_sequences(sequences, maxlen=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "770bf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def prepare_labels(df: DataFrame, label_column_name: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts labels from a PySpark DataFrame and converts them into a NumPy array for model training.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: PySpark DataFrame containing the dataset.\n",
    "    - label_column_name: The name of the column in df that contains the binary labels.\n",
    "    \n",
    "    Returns:\n",
    "    - labels: A NumPy array of labels.\n",
    "    \"\"\"\n",
    "    # Extract labels from the specified column and convert to a list\n",
    "    labels_list = df.select(label_column_name).rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Convert the list of labels to a NumPy array\n",
    "    labels = np.array(labels_list)\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de500059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Email Classification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load your data from the file into a DataFrame\n",
    "# Replace \"/path/to/data_batch_00\" with the actual HDFS path where your file is stored\n",
    "email_df = spark.read.csv(\"hdfs:///user1/data_batch_00\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a1089cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a dummy 'is_suspicious' column with all values set to 0\n",
    "email_df = email_df.withColumn(\"is_suspicious\", lit(0))\n",
    "\n",
    "# Now, you can call the prepare_labels function as before\n",
    "labels = prepare_labels(email_df, \"is_suspicious\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7e5b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def prepare_labels(df: DataFrame, label_column_name: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts labels from a PySpark DataFrame and converts them into a NumPy array for model training.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: PySpark DataFrame containing the dataset.\n",
    "    - label_column_name: The name of the column in df that contains the binary labels.\n",
    "    \n",
    "    Returns:\n",
    "    - labels: A NumPy array of labels.\n",
    "    \"\"\"\n",
    "    # Extract labels from the specified column and convert to a list\n",
    "    labels_list = df.select(label_column_name).rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Convert the list of labels to a NumPy array\n",
    "    labels = np.array(labels_list)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Assuming your DataFrame `email_df` has a binary label column named \"is_suspicious\"\n",
    "labels = prepare_labels(email_df, \"is_suspicious\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e12ed5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773905 2649686\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0cea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(email_texts)\n",
    "X = pad_sequences(sequences, maxlen=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40ee477f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `your_label_column` cannot be resolved. Did you mean one of the following? [`file`, `message`].;\n'Project ['your_label_column]\n+- Filter isnotnull(message#179)\n   +- Relation [file#178,message#179] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4891/2571688387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Ensure labels are extracted from the same filtered/processed DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"your_label_column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4891/1872683189.py\u001b[0m in \u001b[0;36mprepare_labels\u001b[0;34m(df, label_column_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Extract labels from the specified column and convert to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_column_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Convert the list of labels to a NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3034\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m         \"\"\"\n\u001b[0;32m-> 3036\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `your_label_column` cannot be resolved. Did you mean one of the following? [`file`, `message`].;\n'Project ['your_label_column]\n+- Filter isnotnull(message#179)\n   +- Relation [file#178,message#179] csv\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and it's already been filtered/processed as needed\n",
    "\n",
    "# Process df to create X (this should be your existing logic for tokenization and padding)\n",
    "# [Place your logic here for creating X]\n",
    "\n",
    "# Ensure labels are extracted from the same filtered/processed DataFrame\n",
    "labels = prepare_labels(df, \"your_label_column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c2f113a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [773905, 2649686]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4891/824814192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming X is your feature matrix and labels is the numpy array you prepared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2655\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [773905, 2649686]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your feature matrix and labels is the numpy array you prepared\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ce199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13efda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd9f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfbe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42e6c23c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4891/3151768871.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Example preprocessing steps\n",
    "# Load your text data into `texts` and corresponding labels into `labels`\n",
    "# For demonstration, let's assume `texts` is a list of email bodies and `labels` is a binary list indicating suspicious (1) or not (0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Conv1D(filters=32, kernel_size=7, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, np.array(labels), test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e05106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d9780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f047aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Hashing term frequency\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(df_cleaned)\n",
    "\n",
    "# Inverse document frequency\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aca662e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '1'.(line 1, pos 37)\n\n== SQL ==\nCASE WHEN <your_condition_here> THEN 1 ELSE 0 END\n-------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4891/2643752859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_with_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CASE WHEN <your_condition_here> THEN 1 ELSE 0 END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mexpr\u001b[0;34m(str)\u001b[0m\n\u001b[1;32m   3669\u001b[0m     \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3670\u001b[0m     \"\"\"\n\u001b[0;32m-> 3671\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '1'.(line 1, pos 37)\n\n== SQL ==\nCASE WHEN <your_condition_here> THEN 1 ELSE 0 END\n-------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "# Example of adding a binary label column, this is just a placeholder.\n",
    "# You need to replace it with your logic for identifying suspicious emails.\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_with_labels = df.withColumn(\"label\", expr(\"CASE WHEN <your_condition_here> THEN 1 ELSE 0 END\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4efdbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "label does not exist. Available: file, message, words, filtered_words, rawFeatures, features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4891/1076705138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilayerPerceptronClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: file, message, words, filtered_words, rawFeatures, features"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming `is_suspicious` is your column indicating suspicious (1) or not (0)\n",
    "# Rename this column to `label` for use in ML models\n",
    "df_labeled = rescaledData.withColumnRenamed(\"is_suspicious\", \"label\")\n",
    "\n",
    "# Proceed with your existing train-test split and model training\n",
    "splits = df_labeled.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "layers = [10000, 500, 100, 2]\n",
    "\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "model = trainer.fit(train)\n",
    "\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc63182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133a056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fced0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe80c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b0193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5253b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe463ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56da4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b455da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 08:06:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigData\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a6244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dffaa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "df_emails.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda0b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|summary|                file| message|\n",
      "+-------+--------------------+--------+\n",
      "|  count|             8299853| 2508249|\n",
      "|   mean|                 NaN|Infinity|\n",
      "| stddev|                 NaN|     NaN|\n",
      "|    min|                  \\t|      \\t|\n",
      "|    max|~~~~~~~~~~~~~~~~~...|       ||\n",
      "+-------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe provides summary statistics of numeric columns in a DataFrame\n",
    "df_emails.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3e61cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows\n",
    "df_emails.show(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a150d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|ExtractedDate|\n",
      "+-------------+\n",
      "|             |\n",
      "|             |\n",
      "|null         |\n",
      "|null         |\n",
      "|null         |\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Example regular expression pattern for a date in the format \"E, dd MMM yyyy HH:mm:ss Z\"\n",
    "# Adjust this pattern to match the actual format found in your 'message' data\n",
    "date_pattern = r'\\bMon, \\d{2} \\w{3} \\d{4} \\d{2}:\\d{2}:\\d{2} -\\d{4} \\(PDT\\)'\n",
    "\n",
    "# Create a new column 'ExtractedDate' by extracting the date string from 'message'\n",
    "df_emails = df_emails.withColumn(\"ExtractedDate\", regexp_extract(\"message\", date_pattern, 0))\n",
    "\n",
    "# Show the result of extraction\n",
    "df_emails.select(\"ExtractedDate\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75883be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c7f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|message_clean               |\n",
      "+----------------------------+\n",
      "|messageid javamailevansthyme|\n",
      "| may pdt                    |\n",
      "|null                        |\n",
      "|null                        |\n",
      "|null                        |\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Assuming SparkSession has already been created\n",
    "# spark = SparkSession.builder.appName(\"EmailsAnalysis\").getOrCreate()\n",
    "\n",
    "# Make sure df_emails is properly defined here\n",
    "# df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "\n",
    "# Convert the text to lower case\n",
    "df_emails = df_emails.withColumn(\"message_clean\", lower(col(\"message\")))\n",
    "\n",
    "# Remove email metadata, non-letter characters, and extra spaces\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"^(From:|To:|Subject:|Mime-Version:|Content-Type:|Content-Transfer-Encoding:|X-From:|X-To:|X-cc:|X-bcc:|X-Folder:|X-Origin:|X-FileName:).*\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"\\s+\", \" \"))\n",
    "\n",
    "# Show the cleaned text\n",
    "df_emails.select(\"message_clean\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bdb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 08:07:47 WARN DAGScheduler: Broadcasting large task binary with size 1803.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[0,70,202,263,330,474,578,646,680,728,831,886,1184,1250,1264,1369,2071,2266,2742,2840,7311,8723,10224,15635,40555],[0.1015382270840639,6.019066583853422,6.743844934454249,6.919012653891585,7.078540971674206,7.323663429707191,7.479953147783199,7.564825486524079,7.596398748440169,7.67824741627145,7.7866102101032455,7.839385024085388,8.11824450792199,8.175243951886396,8.183864694930303,8.30816241160788,8.67280552519579,8.740958413433015,8.956573698326434,9.004582917512794,10.16054650265237,10.420829600916036,10.647357250214487,11.259158791320479,12.64545315244037])|\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[0,2844],[0.1015382270840639,9.004582917512794])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|(115235,[0,17,5426],[0.1015382270840639,5.08948268439694,9.769067636518946])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 08:08:18 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, to_timestamp\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ensure df_emails is correctly defined and available at this point in your code\n",
    "\n",
    "# Filter out rows where 'message_clean' is null or an empty string\n",
    "df_emails_filtered = df_emails.filter(col(\"message_clean\").isNotNull() & (col(\"message_clean\") != \"\"))\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"message_clean\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
    "\n",
    "# Apply the pipeline to the filtered DataFrame\n",
    "model = pipeline.fit(df_emails_filtered)\n",
    "result = model.transform(df_emails_filtered)\n",
    "\n",
    "# Show the result\n",
    "result.select(\"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271cb90",
   "metadata": {},
   "source": [
    "##### Your model successfully transformed the textual data into numerical vectors that can be used for machine learning purposes, including neural network models for detecting suspicious messages. The warning about broadcasting a large task binary size is an indication of the data size being processed but is generally not a concern unless it leads to performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee82cb",
   "metadata": {},
   "source": [
    "## Step 4: Designing the Neural Network which come  Before moving on to training a neural network model, we'll need to prepare your dataset further, including splitting it into training and test sets, and potentially normalizing the feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42b57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6889c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 08:08:19 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:09:28 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:36 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:36 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:37 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:38 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:39 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:39 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:40 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:40 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:41 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:42 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:43 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:43 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/22 08:10:45 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.9982875492536734\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Column `words` has a data type of array<string>, which is not supported by CSV.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4952/3366385586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Save the predictions to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Replace 'path/to/save/predictions.csv' with the actual path where you want to save the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/data/predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         )\n\u001b[0;32m-> 1799\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m     def orc(\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column `words` has a data type of array<string>, which is not supported by CSV."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Example UDF to label emails based on the presence of a \"suspicious keyword\"\n",
    "def label_email(content):\n",
    "    if content is not None and \"cash\" in content:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Register the UDF\n",
    "label_udf = udf(label_email, IntegerType())\n",
    "\n",
    "# Assuming 'result' is your DataFrame and 'message_clean' is the column containing the cleaned email text\n",
    "# Apply the UDF to create a new column 'label'\n",
    "result = result.withColumn('label', label_udf(col('message_clean')))\n",
    "\n",
    "# Now proceed with data preparation steps such as splitting the dataset into training and test sets\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Ensure your model training code below is correctly referring to 'features' and 'label' columns\n",
    "# For example:\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the classifier, assuming 'features' column contains vectorized features\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lrModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model if necessary\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "# Replace 'path/to/save/predictions.csv' with the actual path where you want to save the file\n",
    "predictions.write.option(\"header\", \"true\").csv('/mnt/data/predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "# For example, let's assume you've loaded and prepared your datasets into these variables\n",
    "# X_train, X_test, y_train, y_test = load_and_preprocess_your_data()\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000,  # Size of your vocabulary\n",
    "              output_dim=128,  # Dimension of the dense embedding\n",
    "              input_length=100),  # Length of input sequences\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c89901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),  # Increased complexity and added dropout\n",
    "    LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Using RMSprop optimizer and setting a learning rate\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a811d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067525e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f43be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill None values with an empty string in the 'message_clean' column\n",
    "df_emails_pd['message_clean'] = df_emails_pd['message_clean'].fillna('')\n",
    "\n",
    "# Now proceed with fitting the tokenizer\n",
    "tokenizer.fit_on_texts(df_emails_pd['message_clean'])\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "\n",
    "# Assuming 'message' is the column with email content that you want to preprocess and predict\n",
    "df_emails = df_emails.withColumnRenamed('message', 'message_clean')  # Only if renaming is necessary\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for processing with Keras\n",
    "df_emails_pd = df_emails.toPandas()\n",
    "\n",
    "# Initialize and fit the tokenizer on the 'message_clean' column\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_emails_pd['message_clean'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df_emails_pd['message_clean'])\n",
    "\n",
    "# Pad sequences\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Add predictions back to the Pandas DataFrame (as a new column)\n",
    "df_emails_pd['fraud_probability'] = predictions\n",
    "\n",
    "# If you want to work with Spark DataFrame afterward, convert it back\n",
    "df_emails_updated = spark.createDataFrame(df_emails_pd)\n",
    "\n",
    "# Show some predictions\n",
    "df_emails_updated.select('message_clean', 'fraud_probability').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# Load your saved model\n",
    "model = load_model('path_to_my_model.h5')\n",
    "\n",
    "# Assuming df_emails is a Pandas DataFrame and has been preprocessed\n",
    "# If df_emails is a Spark DataFrame, you'll need to convert it to Pandas DataFrame\n",
    "# df_emails = spark_df.toPandas()  # Only if starting with a Spark DataFrame\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "# These should match the parameters used during training\n",
    "max_words = 10000  # This should match the tokenizer setting during training\n",
    "maxlen = 100  # This should match the sequence padding length during training\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_emails['message_clean'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df_emails['message_clean'])\n",
    "\n",
    "# Pad sequences\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Add predictions back to the DataFrame (as a new column)\n",
    "df_emails['fraud_probability'] = predictions\n",
    "\n",
    "# Visualize some of the predictions\n",
    "print(df_emails[['message_clean', 'fraud_probability']].head())\n",
    "\n",
    "# If you need to convert back to Spark DataFrame\n",
    "# df_emails_spark = spark.createDataFrame(df_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e439d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fba37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478039e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287c937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d7995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02add97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Data Frame Linux Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947087a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the CSV into 4 parts with a prefix 'data_chunk_'\n",
    "# The -n l/4 option splits the file into 4 equal parts, by lines\n",
    "#This will create files named data_chunk_00, data_chunk_01,\n",
    "\n",
    "###   split -n l/10 -d your_large_file.csv data_chunk_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmailClassification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Replace the path below with the path to your Hadoop-stored file\n",
    "df_emails = spark.read.csv(\"hdfs:///user1/data_batch_00\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2126dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "# Assuming the text column is named 'message'\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_emails)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Continue with further processing as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da825b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Assuming 'df_emails' is your initial DataFrame and 'message' is the column with email texts\n",
    "# Ensure 'df_emails' has been defined correctly and contains the 'message' column\n",
    "\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_emails)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Check if the 'filtered_words' column exists\n",
    "filtered_df.printSchema()\n",
    "\n",
    "# If everything is correct up to here, then converting to a Pandas DataFrame should work\n",
    "pandas_df = filtered_df.select(\"filtered_words\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Filter out rows where the message column is null\n",
    "df_non_null = df_emails.filter(col(\"message\").isNotNull())\n",
    "\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_non_null)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Continue with your processing...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "pandas_df = filtered_df.select(\"filtered_words\").toPandas()\n",
    "\n",
    "# Now, use Keras' text processing tools as shown previously to tokenize and pad sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de264bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Placeholder for your data\n",
    "texts = [\"Your preprocessed email texts here...\"]  # This should be a list of email texts\n",
    "labels = [0, 1, 0, 1]  # Binary labels for each text\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)  # num_words is the size of your vocabulary\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)  # maxlen is the length of the longest sequence\n",
    "\n",
    "# Assuming labels are already prepared\n",
    "import numpy as np\n",
    "labels = np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e201322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example placeholder, replace these with actual preprocessed data and labels\n",
    "texts = [\"sample text 1\", \"sample text 2\", \"sample text 3\", \"sample text 4\"]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)  # Ensure this matches your sequence length\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Now proceed with train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=16, input_length=100),  # input_dim is the size of the vocabulary, output_dim is the dimension of the dense embedding\n",
    "    GlobalAveragePooling1D(),  # This will average the embeddings of all words in the sequence\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification (0 or 1 labels)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786dc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=100, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd65014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb1417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacf2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa222a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdea37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f5cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8aed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f74a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786ff41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdc174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bce61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdda03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a53c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffcdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795585fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad373248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3c67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9493ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37106002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abbcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59fc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54143c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9812fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd12ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dc0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f1b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004be08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50146c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b25a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08912385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ff6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9930f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b26313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f58620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8629c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75c970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17e8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66ebcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c3109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b61bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee443af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32528d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26331c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96545a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5949c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0dfa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa3927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced810d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a91b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd28b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab87719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a04e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b4e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b2cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02541bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2b0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16c620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedf085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca78655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e3c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d886b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c47354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f3743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f669ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac73272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bf171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe21f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fraud_predictions(email_texts, predictions):\n",
    "    \"\"\"\n",
    "    Visualize the emails with their predicted fraud probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - email_texts: List of email text content.\n",
    "    - predictions: List of predicted probabilities corresponding to the fraud likelihood of each email.\n",
    "\n",
    "    The function doesn't return anything but prints each email with its fraud prediction.\n",
    "    \"\"\"\n",
    "    # Ensure the lists have the same length\n",
    "    if len(email_texts) != len(predictions):\n",
    "        print(\"Error: The length of email_texts and predictions must match.\")\n",
    "        return\n",
    "    \n",
    "    for email, probability in zip(email_texts, predictions):\n",
    "        print(\"Email Content:\\n\", email)\n",
    "        # Assuming the probability is given in a scale from 0 to 1\n",
    "        print(\"Fraud Likelihood: {:.2%}\".format(probability))\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# Assuming the predictions list matches the number of email_texts provided\n",
    "predicted_probabilities = [0.34725702, 0.08264993]\n",
    "\n",
    "# Example usage with matching lengths for email_texts and predicted_probabilities\n",
    "visualize_fraud_predictions(email_texts, predicted_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a3b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f68a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emails_by_similarity_and_likelihood(emails, similarity_threshold=0.5, likelihood_threshold=8.0):\n",
    "    \"\"\"\n",
    "    Filters emails based on content similarity to a given phrase and a likelihood threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - emails: List of dictionaries, where each dictionary contains 'content' and 'likelihood' keys.\n",
    "    - similarity_threshold: A threshold for determining content similarity (not used in this simple example).\n",
    "    - likelihood_threshold: The minimum likelihood score for an email to be considered suspicious.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of emails considered suspicious based on the likelihood threshold.\n",
    "    \"\"\"\n",
    "    suspicious_phrase = \"Another suspicious email detected!\"\n",
    "    filtered_emails = [email for email in emails if suspicious_phrase in email['content'] and email['likelihood'] >= likelihood_threshold]\n",
    "    return filtered_emails\n",
    "\n",
    "# Example usage:\n",
    "emails = [\n",
    "    {'content': \"This is a normal email content.\", 'likelihood': 2.0},\n",
    "    {'content': \"Another suspicious email detected! Please check it out.\", 'likelihood': 8.26},\n",
    "    {'content': \"Another suspicious email detected! This seems like a scam.\", 'likelihood': 9.5},\n",
    "    {'content': \"This is another normal conversation.\", 'likelihood': 3.2}\n",
    "]\n",
    "\n",
    "# Filtering emails:\n",
    "suspicious_emails = filter_emails_by_similarity_and_likelihood(emails, likelihood_threshold=8.0)\n",
    "\n",
    "# Displaying the filtered, suspicious emails:\n",
    "for email in suspicious_emails:\n",
    "    print(f\"Email Content: {email['content']}\")\n",
    "    print(f\"Fraud Likelihood: {email['likelihood']}%\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73ec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd9ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f113b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa12248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74514c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a04849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f426539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc31de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26079f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5844a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde65d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169719d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85238ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df98e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
