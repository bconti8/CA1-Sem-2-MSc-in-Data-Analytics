{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa906bd1",
   "metadata": {},
   "source": [
    "### PySpark reading a file form a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38537e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b455da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:04:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigData\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a6244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dffaa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "df_emails.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda0b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|summary|                file| message|\n",
      "+-------+--------------------+--------+\n",
      "|  count|             8299853| 2508249|\n",
      "|   mean|                 NaN|Infinity|\n",
      "| stddev|                 NaN|     NaN|\n",
      "|    min|                  \\t|      \\t|\n",
      "|    max|~~~~~~~~~~~~~~~~~...|       ||\n",
      "+-------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe provides summary statistics of numeric columns in a DataFrame\n",
    "df_emails.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3e61cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows\n",
    "df_emails.show(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a150d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|ExtractedDate|\n",
      "+-------------+\n",
      "|             |\n",
      "|             |\n",
      "|null         |\n",
      "|null         |\n",
      "|null         |\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Example regular expression pattern for a date in the format \"E, dd MMM yyyy HH:mm:ss Z\"\n",
    "# Adjust this pattern to match the actual format found in your 'message' data\n",
    "date_pattern = r'\\bMon, \\d{2} \\w{3} \\d{4} \\d{2}:\\d{2}:\\d{2} -\\d{4} \\(PDT\\)'\n",
    "\n",
    "# Create a new column 'ExtractedDate' by extracting the date string from 'message'\n",
    "df_emails = df_emails.withColumn(\"ExtractedDate\", regexp_extract(\"message\", date_pattern, 0))\n",
    "\n",
    "# Show the result of extraction\n",
    "df_emails.select(\"ExtractedDate\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75883be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c7f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|message_clean               |\n",
      "+----------------------------+\n",
      "|messageid javamailevansthyme|\n",
      "| may pdt                    |\n",
      "|null                        |\n",
      "|null                        |\n",
      "|null                        |\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Assuming SparkSession has already been created\n",
    "# spark = SparkSession.builder.appName(\"EmailsAnalysis\").getOrCreate()\n",
    "\n",
    "# Make sure df_emails is properly defined here\n",
    "# df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "\n",
    "# Convert the text to lower case\n",
    "df_emails = df_emails.withColumn(\"message_clean\", lower(col(\"message\")))\n",
    "\n",
    "# Remove email metadata, non-letter characters, and extra spaces\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"^(From:|To:|Subject:|Mime-Version:|Content-Type:|Content-Transfer-Encoding:|X-From:|X-To:|X-cc:|X-bcc:|X-Folder:|X-Origin:|X-FileName:).*\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"\\s+\", \" \"))\n",
    "\n",
    "# Show the cleaned text\n",
    "df_emails.select(\"message_clean\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bdb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:05:20 WARN DAGScheduler: Broadcasting large task binary with size 1803.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[0,70,202,263,330,474,578,646,679,728,831,886,1184,1250,1263,1369,2070,2266,2745,2839,7311,8718,10224,15716,40583],[0.1015382270840639,6.019066583853422,6.743844934454249,6.919012653891585,7.078540971674206,7.323663429707191,7.479953147783199,7.564825486524079,7.596398748440169,7.67824741627145,7.7866102101032455,7.839385024085388,8.11824450792199,8.175243951886396,8.183864694930303,8.30816241160788,8.67280552519579,8.740958413433015,8.956573698326434,9.004582917512794,10.16054650265237,10.420829600916036,10.647357250214487,11.259158791320479,12.64545315244037])|\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[0,2841],[0.1015382270840639,9.004582917512794])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|(115235,[0,17,5423],[0.1015382270840639,5.08948268439694,9.769067636518946])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:05:47 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, to_timestamp\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ensure df_emails is correctly defined and available at this point in your code\n",
    "\n",
    "# Filter out rows where 'message_clean' is null or an empty string\n",
    "df_emails_filtered = df_emails.filter(col(\"message_clean\").isNotNull() & (col(\"message_clean\") != \"\"))\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"message_clean\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
    "\n",
    "# Apply the pipeline to the filtered DataFrame\n",
    "model = pipeline.fit(df_emails_filtered)\n",
    "result = model.transform(df_emails_filtered)\n",
    "\n",
    "# Show the result\n",
    "result.select(\"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ec3d2",
   "metadata": {},
   "source": [
    "##### Your model successfully transformed the textual data into numerical vectors that can be used for machine learning purposes, including neural network models for detecting suspicious messages. The warning about broadcasting a large task binary size is an indication of the data size being processed but is generally not a concern unless it leads to performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10209b",
   "metadata": {},
   "source": [
    "## Step 4: Designing the Neural Network which come  Before moving on to training a neural network model, we'll need to prepare your dataset further, including splitting it into training and test sets, and potentially normalizing the feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f150b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faddfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5d8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:05:48 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:06:50 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:51 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:51 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:52 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:52 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:53 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:53 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:53 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:54 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:54 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:55 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:55 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:55 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:07:57 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.9982872399017843\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Example UDF to label emails based on the presence of a \"suspicious keyword\"\n",
    "def label_email(content):\n",
    "    if content is not None and \"cash\" in content:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Register the UDF\n",
    "label_udf = udf(label_email, IntegerType())\n",
    "\n",
    "# Assuming 'result' is your DataFrame and 'message_clean' is the column containing the cleaned email text\n",
    "# Apply the UDF to create a new column 'label'\n",
    "result = result.withColumn('label', label_udf(col('message_clean')))\n",
    "\n",
    "# Now proceed with data preparation steps such as splitting the dataset into training and test sets\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Ensure your model training code below is correctly referring to 'features' and 'label' columns\n",
    "# For example:\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the classifier, assuming 'features' column contains vectorized features\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lrModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model if necessary\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1bf878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 3s 126ms/step - loss: 0.6932 - accuracy: 0.5038 - val_loss: 0.6938 - val_accuracy: 0.4650\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.6761 - accuracy: 0.6900 - val_loss: 0.6966 - val_accuracy: 0.4650\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.5790 - accuracy: 0.8025 - val_loss: 0.7353 - val_accuracy: 0.4750\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 0.2388 - accuracy: 0.9787 - val_loss: 0.8172 - val_accuracy: 0.5150\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0824 - accuracy: 0.9900 - val_loss: 1.3673 - val_accuracy: 0.4750\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 0.0294 - accuracy: 0.9987 - val_loss: 1.3863 - val_accuracy: 0.4700\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 1.7193 - val_accuracy: 0.4450\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.5533 - val_accuracy: 0.4500\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.7368 - val_accuracy: 0.4350\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.4427 - val_accuracy: 0.4600\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.6418 - val_accuracy: 0.4550\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.2220 - val_accuracy: 0.4350\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.1756 - val_accuracy: 0.4400\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.9799 - val_accuracy: 0.4500\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.0613 - val_accuracy: 0.4400\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5484 - val_accuracy: 0.4600\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8196 - val_accuracy: 0.4700\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.8511 - val_accuracy: 0.4600\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.8821 - val_accuracy: 0.4650\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9555 - val_accuracy: 0.4650\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.0201 - val_accuracy: 0.4700\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 9.8450e-04 - accuracy: 1.0000 - val_loss: 3.1053 - val_accuracy: 0.4500\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 8.2165e-04 - accuracy: 1.0000 - val_loss: 3.1350 - val_accuracy: 0.4500\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 8.1188e-04 - accuracy: 1.0000 - val_loss: 3.1513 - val_accuracy: 0.4700\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 6.9158e-04 - accuracy: 1.0000 - val_loss: 3.1962 - val_accuracy: 0.4700\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 6.6879e-04 - accuracy: 1.0000 - val_loss: 3.2290 - val_accuracy: 0.4700\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 6.9129e-04 - accuracy: 1.0000 - val_loss: 3.2857 - val_accuracy: 0.4650\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5.9298e-04 - accuracy: 1.0000 - val_loss: 3.3218 - val_accuracy: 0.4700\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5.9056e-04 - accuracy: 1.0000 - val_loss: 3.3440 - val_accuracy: 0.4650\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5.5071e-04 - accuracy: 1.0000 - val_loss: 3.3804 - val_accuracy: 0.4650\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5.3821e-04 - accuracy: 1.0000 - val_loss: 3.3789 - val_accuracy: 0.4700\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 5.5692e-04 - accuracy: 1.0000 - val_loss: 3.3884 - val_accuracy: 0.4700\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 4.3851e-04 - accuracy: 1.0000 - val_loss: 3.4259 - val_accuracy: 0.4700\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 5.0278e-04 - accuracy: 1.0000 - val_loss: 3.4614 - val_accuracy: 0.4750\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 4.3968e-04 - accuracy: 1.0000 - val_loss: 3.5205 - val_accuracy: 0.4750\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 4.1243e-04 - accuracy: 1.0000 - val_loss: 3.5550 - val_accuracy: 0.4600\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 3.8568e-04 - accuracy: 1.0000 - val_loss: 3.5847 - val_accuracy: 0.4700\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 3.9180e-04 - accuracy: 1.0000 - val_loss: 3.6124 - val_accuracy: 0.4650\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 3.8767e-04 - accuracy: 1.0000 - val_loss: 3.6276 - val_accuracy: 0.4700\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 2s 137ms/step - loss: 3.1519e-04 - accuracy: 1.0000 - val_loss: 3.6576 - val_accuracy: 0.4700\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 3.4862e-04 - accuracy: 1.0000 - val_loss: 3.6619 - val_accuracy: 0.4700\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 3.1145e-04 - accuracy: 1.0000 - val_loss: 3.6716 - val_accuracy: 0.4700\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 3.0436e-04 - accuracy: 1.0000 - val_loss: 3.7152 - val_accuracy: 0.4700\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 3.3339e-04 - accuracy: 1.0000 - val_loss: 3.7438 - val_accuracy: 0.4700\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 2.8741e-04 - accuracy: 1.0000 - val_loss: 3.7999 - val_accuracy: 0.4650\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 2.5516e-04 - accuracy: 1.0000 - val_loss: 3.7879 - val_accuracy: 0.4700\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 2.4764e-04 - accuracy: 1.0000 - val_loss: 3.7658 - val_accuracy: 0.4650\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 2.4666e-04 - accuracy: 1.0000 - val_loss: 3.7876 - val_accuracy: 0.4600\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 2.5382e-04 - accuracy: 1.0000 - val_loss: 3.8143 - val_accuracy: 0.4650\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 2.2674e-04 - accuracy: 1.0000 - val_loss: 3.8409 - val_accuracy: 0.4700\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3.5182 - accuracy: 0.5200\n",
      "Test Accuracy: 0.5199999809265137\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "# For example, let's assume you've loaded and prepared your datasets into these variables\n",
    "# X_train, X_test, y_train, y_test = load_and_preprocess_your_data()\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000,  # Size of your vocabulary\n",
    "              output_dim=128,  # Dimension of the dense embedding\n",
    "              input_length=100),  # Length of input sequences\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e914e135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 7s 291ms/step - loss: 0.6942 - accuracy: 0.4563 - val_loss: 0.6918 - val_accuracy: 0.5400\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6921 - val_accuracy: 0.5400\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 3s 259ms/step - loss: 0.6917 - accuracy: 0.5250 - val_loss: 0.6927 - val_accuracy: 0.5450\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 3s 247ms/step - loss: 0.6861 - accuracy: 0.6400 - val_loss: 0.6896 - val_accuracy: 0.5400\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 3s 243ms/step - loss: 0.6622 - accuracy: 0.6913 - val_loss: 0.6964 - val_accuracy: 0.5200\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.7214 - accuracy: 0.4650\n",
      "Test Accuracy: 0.4650000035762787\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),  # Increased complexity and added dropout\n",
    "    LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Using RMSprop optimizer and setting a learning rate\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42c3cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11f694c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o70.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4438/1657540594.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset$$Lambda$4436/1774544996.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.Dataset$$Lambda$1997/1826484949.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset$$Lambda$1658/1223871856.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1672/529568201.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1659/664329173.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13173/3107171921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert Spark DataFrame to Pandas DataFrame for Keras compatibility (if starting from Spark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf_emails_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_emails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Tokenize and pad the email texts (now using Keras Tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o70.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4438/1657540594.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset$$Lambda$4436/1774544996.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.Dataset$$Lambda$1997/1826484949.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset$$Lambda$1658/1223871856.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1672/529568201.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1659/664329173.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a loaded Keras Tokenizer and your model\n",
    "# tokenizer = ...\n",
    "model = load_model('path_to_my_model.h5')\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for Keras compatibility (if starting from Spark)\n",
    "df_emails_pd = df_emails.toPandas()\n",
    "\n",
    "# Tokenize and pad the email texts (now using Keras Tokenizer)\n",
    "sequences = tokenizer.texts_to_sequences(df_emails_pd['message_clean'])\n",
    "data = pad_sequences(sequences, maxlen=max_length)  # Ensure `max_length` matches your training data's sequence length\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Interpret predictions and add to DataFrame\n",
    "df_emails_pd['Classification'] = ['Suspicious email detected!' if pred > 0.5 else 'Normal conversation.' for pred in predictions.flatten()]\n",
    "\n",
    "# If you need to work with Spark DataFrames afterward, you can convert back\n",
    "df_emails_updated = spark.createDataFrame(df_emails_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd18d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26b9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1180abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ff0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0777fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2d06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8c821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50263f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6cb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10590e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30bced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cef637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeb434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b59f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c54070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e41ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c59f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8c4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be94301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004be08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50146c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b25a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08912385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7998bbf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ff6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9930f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b26313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f58620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8629c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75c970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17e8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66ebcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c3109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b61bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee443af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32528d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26331c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96545a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5949c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0dfa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa3927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced810d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a91b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab87719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a04e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b2cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02541bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_message(message):\n",
    "    # Initialize a dictionary to hold the parsed data\n",
    "    parsed_data = {\n",
    "        \"MessageID\": \"\",\n",
    "        \"Date\": \"\",\n",
    "        \"From\": \"\",\n",
    "        \"To\": \"\",\n",
    "        \"Subject\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Split the message into lines for processing\n",
    "    lines = message.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Message-ID:\"):\n",
    "            parsed_data[\"MessageID\"] = line[len(\"Message-ID:\"):].strip()\n",
    "        elif line.startswith(\"Date:\"):\n",
    "            parsed_data[\"Date\"] = line[len(\"Date:\"):].strip()\n",
    "        elif line.startswith(\"From:\"):\n",
    "            parsed_data[\"From\"] = line[len(\"From:\"):].strip()\n",
    "        elif line.startswith(\"To:\"):\n",
    "            parsed_data[\"To\"] = line[len(\"To:\"):].strip()\n",
    "        elif line.startswith(\"Subject:\"):\n",
    "            parsed_data[\"Subject\"] = line[len(\"Subject:\"):].strip()\n",
    "        # Add more conditions as needed for other fields\n",
    "    \n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_10_messages(df):\n",
    "    # Assuming 'df' is your DataFrame and it has a column named 'message'\n",
    "    # This will show the first 10 rows of the 'message' column\n",
    "    df.select(\"message\").show(10, truncate=False)\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "show_first_10_messages(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define your schema as before\n",
    "schema = StructType([\n",
    "    StructField(\"MessageID\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"From\", StringType(), True),\n",
    "    StructField(\"To\", StringType(), True),\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    # Add other fields as necessary\n",
    "])\n",
    "\n",
    "def parse_message(message):\n",
    "    # Initialize the dictionary with default empty strings\n",
    "    parsed_data = {\n",
    "        \"MessageID\": \"\",\n",
    "        \"Date\": \"\",\n",
    "        \"From\": \"\",\n",
    "        \"To\": \"\",\n",
    "        \"Subject\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Proceed only if message is not None and is a string\n",
    "    if message and isinstance(message, str):\n",
    "        lines = message.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Message-ID:\"):\n",
    "                parsed_data[\"MessageID\"] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"Date:\"):\n",
    "                parsed_data[\"Date\"] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"From:\"):\n",
    "                parsed_data[\"From\"] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"To:\"):\n",
    "                parsed_data[\"To\"] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"Subject:\"):\n",
    "                parsed_data[\"Subject\"] = line.split(\":\", 1)[1].strip()\n",
    "            # Continue with other headers as needed\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "# Register the UDF with the modified parse_message function\n",
    "parse_message_udf = udf(parse_message, schema)\n",
    "\n",
    "# Apply the UDF to your DataFrame as before\n",
    "df_parsed = df.withColumn(\"parsed_message\", parse_message_udf(df[\"message\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_specific_message(df, index):\n",
    "    \"\"\"\n",
    "    Displays a specific message from the DataFrame based on the provided index.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The Spark DataFrame containing the messages.\n",
    "    - index: The index (row number) of the message to display.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has a column named 'message'\n",
    "    if 'message' in df.columns:\n",
    "        # Collect the row of interest into a list\n",
    "        message_row = df.select(\"message\").collect()[index]\n",
    "        \n",
    "        # Extract the message from the row and print it\n",
    "        message_content = message_row[\"message\"]\n",
    "        print(f\"Message at index {index}:\\n{message_content}\")\n",
    "    else:\n",
    "        print(\"The DataFrame does not contain a column named 'message'.\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame and you want to see the first message\n",
    "show_specific_message(df, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16c620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedf085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca78655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Load the dataset with the correct parameters for handling potential parsing issues\n",
    "df_emails = pd.read_csv('emails.csv', quoting=csv.QUOTE_NONE, on_bad_lines='skip', escapechar=\"\\\\\")\n",
    "\n",
    "# Removing quotes from column names if they exist\n",
    "df_emails.columns = df_emails.columns.str.replace('\"', '')\n",
    "\n",
    "# Handling Missing Values\n",
    "df_emails.fillna('', inplace=True)\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove email headers or unnecessary metadata (for demonstration, might need customization)\n",
    "    text = re.sub(r'^[a-z]+:.*$', '', text)  # Remove lines that start with metadata-like patterns\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with single space\n",
    "    # Remove special characters (customize based on the dataset and needs)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply text preprocessing to the 'message' column\n",
    "df_emails['message'] = df_emails['message'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the cleaned dataframe\n",
    "df_emails.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mock example of a structured dataframe\n",
    "data = {\n",
    "    'body': ['This is the first email content.', 'Here is another email, potentially suspicious.', 'This email is safe and informative.']\n",
    "}\n",
    "emails_structured_df = pd.DataFrame(data)\n",
    "\n",
    "# Assuming the vectorization function from the previous message, apply it here:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_texts(texts):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        max_features=10000,\n",
    "        min_df=1,\n",
    "        max_df=0.9\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer\n",
    "\n",
    "# Apply vectorization to the 'body' column of emails_structured_df\n",
    "texts = emails_structured_df['body'].tolist()\n",
    "X, vectorizer = vectorize_texts(texts)\n",
    "\n",
    "print(\"Vectorization Complete. Shape of X:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def evaluate_model(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets, trains a model, and evaluates its performance.\n",
    "    \n",
    "    Parameters:\n",
    "    X (sparse matrix): The feature matrix obtained from vectorizing the text data.\n",
    "    y (array-like): The target labels indicating the class of each document.\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary containing the model's performance metrics: accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "# Assume y is your array of labels for the dataset, with 1 indicating suspicious and 0 indicating not suspicious\n",
    "# y = [1, 0, 1, ...]  # This should be the actual labels for your dataset\n",
    "# metrics = evaluate_model(X, y)\n",
    "# print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c47354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model(input_dim):\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "    # Add layers to the model\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')) # Use 'sigmoid' for binary classification\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'Precision', 'Recall'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Mock data for demonstration purposes\n",
    "# Replace these with your actual vectorized data (X) and labels (y)\n",
    "X = np.random.rand(100, 20)  # Example feature matrix with 100 samples and 20 features\n",
    "y = np.random.randint(2, size=100)  # Example binary labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f669ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',  # Use binary_crossentropy for binary classification\n",
    "                  metrics=['accuracy', 'Precision', 'Recall'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac73272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that X_train is defined, we can proceed to use it\n",
    "input_dim = X_train.shape[1]  # Number of features from the vectorized data\n",
    "model = create_model(input_dim)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example new data\n",
    "new_data = [\"This is a new email conversation.\", \"Another suspicious email detected!\", \"Normal conversation.\"]\n",
    "\n",
    "# Preprocess and vectorize new data\n",
    "new_data_processed = [preprocess_text(text) for text in new_data]  # Using the same preprocess_text function from before\n",
    "new_data_vectorized = vectorizer.transform(new_data_processed)  # Use the same vectorizer fitted on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bf171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_predict(new_data, vectorizer, model):\n",
    "    # Preprocess new data\n",
    "    new_data_processed = [preprocess_text(text) for text in new_data]\n",
    "    \n",
    "    # Vectorize new data using the same vectorizer instance used for training\n",
    "    new_data_vectorized = vectorizer.transform(new_data_processed)\n",
    "    \n",
    "    # IMPORTANT: Artificially ensure the shape matches the expected input of the model\n",
    "    # This step is hypothetical and serves to illustrate the concept\n",
    "    # In practice, ensure your data vectorization matches the training phase accurately\n",
    "    if new_data_vectorized.shape[1] < 20:\n",
    "        # Assuming the missing features can be set to 0 (this is a strong assumption and may not be valid)\n",
    "        additional_zeros = np.zeros((new_data_vectorized.shape[0], 20 - new_data_vectorized.shape[1]))\n",
    "        new_data_vectorized = np.hstack((new_data_vectorized.toarray(), additional_zeros))\n",
    "    \n",
    "    predictions = model.predict(new_data_vectorized)\n",
    "    predicted_probabilities = predictions.flatten()\n",
    "    return predicted_probabilities\n",
    "\n",
    "# Example usage\n",
    "new_data = [\"This is a new email conversation.\", \"Another suspicious email detected!\", \"Normal conversation.\"]\n",
    "predicted_probabilities = prepare_and_predict(new_data, vectorizer, model)\n",
    "print(predicted_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(emails, probabilities):\n",
    "    \"\"\"\n",
    "    Visualizes the predicted probabilities of emails being suspicious or fraudulent.\n",
    "\n",
    "    Parameters:\n",
    "    - emails: A list of email texts or subjects being analyzed.\n",
    "    - probabilities: A list of probabilities corresponding to the likelihood of each email being suspicious.\n",
    "    \"\"\"\n",
    "    # Ensure the lists have the same length\n",
    "    assert len(emails) == len(probabilities), \"Emails and probabilities lists must have the same length.\"\n",
    "\n",
    "    # Creating the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(emails, probabilities, color='skyblue')\n",
    "    plt.xlabel('Probability of Being Suspicious')\n",
    "    plt.title('Predicted Probabilities of Emails Being Suspicious or Fraudulent')\n",
    "    for index, value in enumerate(probabilities):\n",
    "        plt.text(value, index, f\"{value:.2f}\")\n",
    "    plt.xlim(0, 1)  # Assuming probabilities range from 0 to 1\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "emails = [\"New email conversation\", \"Suspicious email detected\", \"Normal conversation\"]\n",
    "probabilities = [0.34725702, 0.08264993, 0.34725702]\n",
    "visualize_predictions(emails, probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fraud_predictions(email_texts, predictions):\n",
    "    \"\"\"\n",
    "    Visualize the emails with their predicted fraud probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - email_texts: List of email text content.\n",
    "    - predictions: List of predicted probabilities corresponding to the fraud likelihood of each email.\n",
    "\n",
    "    The function doesn't return anything but prints each email with its fraud prediction.\n",
    "    \"\"\"\n",
    "    for email, probability in zip(email_texts, predictions):\n",
    "        print(\"Email Content:\\n\", email)\n",
    "        print(\"Fraud Likelihood: {:.2%}\".format(probability))\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# Example usage\n",
    "email_texts = [\n",
    "    \"This is a new email conversation.\",\n",
    "    \"Another suspicious email detected!\",\n",
    "    \"Normal conversation.\"\n",
    "]\n",
    "predicted_probabilities = [0.34725702, 0.08264993, 0.34725702]\n",
    "\n",
    "visualize_fraud_predictions(email_texts, predicted_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a3b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f68a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emails_by_similarity_and_likelihood(emails, similarity_threshold=0.5, likelihood_threshold=8.0):\n",
    "    \"\"\"\n",
    "    Filters emails based on content similarity to a given phrase and a likelihood threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - emails: List of dictionaries, where each dictionary contains 'content' and 'likelihood' keys.\n",
    "    - similarity_threshold: A threshold for determining content similarity (not used in this simple example).\n",
    "    - likelihood_threshold: The minimum likelihood score for an email to be considered suspicious.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of emails considered suspicious based on the likelihood threshold.\n",
    "    \"\"\"\n",
    "    suspicious_phrase = \"Another suspicious email detected!\"\n",
    "    filtered_emails = [email for email in emails if suspicious_phrase in email['content'] and email['likelihood'] >= likelihood_threshold]\n",
    "    return filtered_emails\n",
    "\n",
    "# Example usage:\n",
    "emails = [\n",
    "    {'content': \"This is a normal email content.\", 'likelihood': 2.0},\n",
    "    {'content': \"Another suspicious email detected! Please check it out.\", 'likelihood': 8.26},\n",
    "    {'content': \"Another suspicious email detected! This seems like a scam.\", 'likelihood': 9.5},\n",
    "    {'content': \"This is another normal conversation.\", 'likelihood': 3.2}\n",
    "]\n",
    "\n",
    "# Filtering emails:\n",
    "suspicious_emails = filter_emails_by_similarity_and_likelihood(emails, likelihood_threshold=8.0)\n",
    "\n",
    "# Displaying the filtered, suspicious emails:\n",
    "for email in suspicious_emails:\n",
    "    print(f\"Email Content: {email['content']}\")\n",
    "    print(f\"Fraud Likelihood: {email['likelihood']}%\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73ec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd9ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f113b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa12248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74514c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a04849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f426539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc31de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26079f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5844a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde65d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169719d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85238ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df98e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
