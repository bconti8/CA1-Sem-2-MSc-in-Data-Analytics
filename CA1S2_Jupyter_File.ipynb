{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa906bd1",
   "metadata": {},
   "source": [
    "### PySpark reading a file form a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38537e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b455da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:40:26 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigData\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a6244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dffaa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "df_emails.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda0b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|summary|                file| message|\n",
      "+-------+--------------------+--------+\n",
      "|  count|             8299853| 2508249|\n",
      "|   mean|                 NaN|Infinity|\n",
      "| stddev|                 NaN|     NaN|\n",
      "|    min|                  \\t|      \\t|\n",
      "|    max|~~~~~~~~~~~~~~~~~...|       ||\n",
      "+-------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe provides summary statistics of numeric columns in a DataFrame\n",
    "df_emails.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3e61cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                file|             message|\n",
      "+--------------------+--------------------+\n",
      "|allen-p/_sent_mai...|Message-ID: <1878...|\n",
      "|           Date: Mon| 14 May 2001 16:3...|\n",
      "|From: phillip.all...|                null|\n",
      "|To: tim.belden@en...|                null|\n",
      "|           Subject: |                null|\n",
      "|   Mime-Version: 1.0|                null|\n",
      "|Content-Type: tex...|                null|\n",
      "|Content-Transfer-...|                null|\n",
      "|X-From: Phillip K...|                null|\n",
      "|X-To: Tim Belden ...|                null|\n",
      "|              X-cc: |                null|\n",
      "|             X-bcc: |                null|\n",
      "|X-Folder: \\Philli...| Phillip K.\\'Sent...|\n",
      "|   X-Origin: Allen-P|                null|\n",
      "|X-FileName: palle...|                null|\n",
      "|Here is our forecast|                null|\n",
      "|                   \"|                null|\n",
      "|allen-p/_sent_mai...|Message-ID: <1546...|\n",
      "|           Date: Fri| 4 May 2001 13:51...|\n",
      "|From: phillip.all...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows\n",
    "df_emails.show(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a150d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|ExtractedDate|\n",
      "+-------------+\n",
      "|             |\n",
      "|             |\n",
      "|null         |\n",
      "|null         |\n",
      "|null         |\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Example regular expression pattern for a date in the format \"E, dd MMM yyyy HH:mm:ss Z\"\n",
    "# Adjust this pattern to match the actual format found in your 'message' data\n",
    "date_pattern = r'\\bMon, \\d{2} \\w{3} \\d{4} \\d{2}:\\d{2}:\\d{2} -\\d{4} \\(PDT\\)'\n",
    "\n",
    "# Create a new column 'ExtractedDate' by extracting the date string from 'message'\n",
    "df_emails = df_emails.withColumn(\"ExtractedDate\", regexp_extract(\"message\", date_pattern, 0))\n",
    "\n",
    "# Show the result of extraction\n",
    "df_emails.select(\"ExtractedDate\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75883be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c7f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|message_clean               |\n",
      "+----------------------------+\n",
      "|messageid javamailevansthyme|\n",
      "| may pdt                    |\n",
      "|null                        |\n",
      "|null                        |\n",
      "|null                        |\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Assuming SparkSession has already been created\n",
    "# spark = SparkSession.builder.appName(\"EmailsAnalysis\").getOrCreate()\n",
    "\n",
    "# Make sure df_emails is properly defined here\n",
    "# df_emails = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://localhost:8020/user1/emails.csv\")\n",
    "\n",
    "# Convert the text to lower case\n",
    "df_emails = df_emails.withColumn(\"message_clean\", lower(col(\"message\")))\n",
    "\n",
    "# Remove email metadata, non-letter characters, and extra spaces\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"^(From:|To:|Subject:|Mime-Version:|Content-Type:|Content-Transfer-Encoding:|X-From:|X-To:|X-cc:|X-bcc:|X-Folder:|X-Origin:|X-FileName:).*\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "df_emails = df_emails.withColumn(\"message_clean\", regexp_replace(\"message_clean\", \"\\s+\", \" \"))\n",
    "\n",
    "# Show the cleaned text\n",
    "df_emails.select(\"message_clean\").show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bdb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:41:41 WARN DAGScheduler: Broadcasting large task binary with size 1803.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,6],[0.1015382270840639,3.508759320632486,4.414776046835944])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|(115235,[0,163,503,2295],[0.1015382270840639,6.563234242063924,7.39317972439374,8.751085082250945])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|(115235,[0,70,202,263,330,474,578,646,679,728,831,886,1184,1250,1263,1369,2070,2264,2745,2839,7299,8718,10202,15789,40585],[0.1015382270840639,6.019066583853422,6.743844934454249,6.919012653891585,7.078540971674206,7.323663429707191,7.479953147783199,7.564825486524079,7.596398748440169,7.67824741627145,7.7866102101032455,7.839385024085388,8.11824450792199,8.175243951886396,8.183864694930303,8.30816241160788,8.67280552519579,8.740958413433015,8.956573698326434,9.004582917512794,10.16054650265237,10.420829600916036,10.647357250214487,11.259158791320479,12.64545315244037])|\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,10],[0.1015382270840639,3.508759320632486,4.796714528993706])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|(115235,[0,3,42],[0.1015382270840639,3.508759320632486,5.635479790072903])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|(115235,[0,2841],[0.1015382270840639,9.004582917512794])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|(115235,[0,17,5423],[0.1015382270840639,5.08948268439694,9.769067636518946])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|(115235,[1,2],[2.8276324266613337,2.8276324266613337])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:42:10 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, to_timestamp\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ensure df_emails is correctly defined and available at this point in your code\n",
    "\n",
    "# Filter out rows where 'message_clean' is null or an empty string\n",
    "df_emails_filtered = df_emails.filter(col(\"message_clean\").isNotNull() & (col(\"message_clean\") != \"\"))\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"message_clean\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
    "\n",
    "# Apply the pipeline to the filtered DataFrame\n",
    "model = pipeline.fit(df_emails_filtered)\n",
    "result = model.transform(df_emails_filtered)\n",
    "\n",
    "# Show the result\n",
    "result.select(\"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271cb90",
   "metadata": {},
   "source": [
    "##### Your model successfully transformed the textual data into numerical vectors that can be used for machine learning purposes, including neural network models for detecting suspicious messages. The warning about broadcasting a large task binary size is an indication of the data size being processed but is generally not a concern unless it leads to performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee82cb",
   "metadata": {},
   "source": [
    "## Step 4: Designing the Neural Network which come  Before moving on to training a neural network model, we'll need to prepare your dataset further, including splitting it into training and test sets, and potentially normalizing the feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42b57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6889c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 10:42:11 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:43:13 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:11 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:12 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:12 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:13 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:13 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:15 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:15 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/03/18 10:44:18 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.9982873684951179\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Example UDF to label emails based on the presence of a \"suspicious keyword\"\n",
    "def label_email(content):\n",
    "    if content is not None and \"cash\" in content:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Register the UDF\n",
    "label_udf = udf(label_email, IntegerType())\n",
    "\n",
    "# Assuming 'result' is your DataFrame and 'message_clean' is the column containing the cleaned email text\n",
    "# Apply the UDF to create a new column 'label'\n",
    "result = result.withColumn('label', label_udf(col('message_clean')))\n",
    "\n",
    "# Now proceed with data preparation steps such as splitting the dataset into training and test sets\n",
    "(train_data, test_data) = result.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Ensure your model training code below is correctly referring to 'features' and 'label' columns\n",
    "# For example:\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the classifier, assuming 'features' column contains vectorized features\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lrModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model if necessary\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726f7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 10:45:18.218023: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-18 10:45:18.452547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-18 10:45:18.452641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-18 10:45:18.489650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-18 10:45:18.570062: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-18 10:45:18.573040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 10:45:19.950125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 3s 109ms/step - loss: 0.6932 - accuracy: 0.4875 - val_loss: 0.6930 - val_accuracy: 0.5050\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 0.6741 - accuracy: 0.7700 - val_loss: 0.6933 - val_accuracy: 0.5100\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.5685 - accuracy: 0.8512 - val_loss: 0.6952 - val_accuracy: 0.5700\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 0.2505 - accuracy: 0.9850 - val_loss: 0.8074 - val_accuracy: 0.5400\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.0603 - accuracy: 0.9975 - val_loss: 1.0763 - val_accuracy: 0.5250\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 1.4370 - val_accuracy: 0.5500\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.6529 - val_accuracy: 0.5350\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.7671 - val_accuracy: 0.5450\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 2s 115ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.8608 - val_accuracy: 0.5350\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 2s 151ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.0548 - val_accuracy: 0.5400\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 0.0179 - accuracy: 0.9987 - val_loss: 1.4626 - val_accuracy: 0.5500\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.7245 - val_accuracy: 0.5200\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 1.8747 - val_accuracy: 0.5100\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.6571 - val_accuracy: 0.5500\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 2s 114ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.9152 - val_accuracy: 0.5150\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.1144 - val_accuracy: 0.5100\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.2119 - val_accuracy: 0.5200\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.2665 - val_accuracy: 0.5250\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.3726 - val_accuracy: 0.5300\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4695 - val_accuracy: 0.5250\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.5339 - val_accuracy: 0.5300\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.5889 - val_accuracy: 0.5200\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6326 - val_accuracy: 0.5200\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6808 - val_accuracy: 0.5200\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7188 - val_accuracy: 0.5150\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7549 - val_accuracy: 0.5200\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 2s 122ms/step - loss: 9.5773e-04 - accuracy: 1.0000 - val_loss: 2.8102 - val_accuracy: 0.5200\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 9.1650e-04 - accuracy: 1.0000 - val_loss: 2.8565 - val_accuracy: 0.5200\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 8.8287e-04 - accuracy: 1.0000 - val_loss: 2.8795 - val_accuracy: 0.5250\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 7.3340e-04 - accuracy: 1.0000 - val_loss: 2.9163 - val_accuracy: 0.5250\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 2s 123ms/step - loss: 7.8443e-04 - accuracy: 1.0000 - val_loss: 2.9593 - val_accuracy: 0.5200\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 7.7236e-04 - accuracy: 1.0000 - val_loss: 2.9863 - val_accuracy: 0.5200\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 6.9314e-04 - accuracy: 1.0000 - val_loss: 3.0102 - val_accuracy: 0.5200\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 6.9453e-04 - accuracy: 1.0000 - val_loss: 3.0549 - val_accuracy: 0.5200\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 5.8463e-04 - accuracy: 1.0000 - val_loss: 3.0918 - val_accuracy: 0.5200\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 5.8932e-04 - accuracy: 1.0000 - val_loss: 3.1217 - val_accuracy: 0.5200\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 5.6800e-04 - accuracy: 1.0000 - val_loss: 3.1487 - val_accuracy: 0.5200\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 1s 104ms/step - loss: 4.8086e-04 - accuracy: 1.0000 - val_loss: 3.1753 - val_accuracy: 0.5200\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 4.7250e-04 - accuracy: 1.0000 - val_loss: 3.1989 - val_accuracy: 0.5200\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5.1836e-04 - accuracy: 1.0000 - val_loss: 3.2255 - val_accuracy: 0.5250\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5.4831e-04 - accuracy: 1.0000 - val_loss: 3.2546 - val_accuracy: 0.5250\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 4.5591e-04 - accuracy: 1.0000 - val_loss: 3.2753 - val_accuracy: 0.5250\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 3.8686e-04 - accuracy: 1.0000 - val_loss: 3.2972 - val_accuracy: 0.5250\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 3.8176e-04 - accuracy: 1.0000 - val_loss: 3.3166 - val_accuracy: 0.5250\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 4.1081e-04 - accuracy: 1.0000 - val_loss: 3.3391 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 3.9880e-04 - accuracy: 1.0000 - val_loss: 3.3596 - val_accuracy: 0.5250\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 4.0891e-04 - accuracy: 1.0000 - val_loss: 3.3850 - val_accuracy: 0.5300\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 3.3646e-04 - accuracy: 1.0000 - val_loss: 3.4080 - val_accuracy: 0.5250\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 3.4970e-04 - accuracy: 1.0000 - val_loss: 3.4202 - val_accuracy: 0.5300\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 3.1662e-04 - accuracy: 1.0000 - val_loss: 3.4349 - val_accuracy: 0.5350\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4.2006 - accuracy: 0.4250\n",
      "Test Accuracy: 0.42500001192092896\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "# For example, let's assume you've loaded and prepared your datasets into these variables\n",
    "# X_train, X_test, y_train, y_test = load_and_preprocess_your_data()\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000,  # Size of your vocabulary\n",
    "              output_dim=128,  # Dimension of the dense embedding\n",
    "              input_length=100),  # Length of input sequences\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c89901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 7s 276ms/step - loss: 0.6934 - accuracy: 0.5013 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 3s 245ms/step - loss: 0.6917 - accuracy: 0.5275 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 3s 249ms/step - loss: 0.6908 - accuracy: 0.5350 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 3s 256ms/step - loss: 0.6903 - accuracy: 0.5275 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 4s 283ms/step - loss: 0.6842 - accuracy: 0.5625 - val_loss: 0.7013 - val_accuracy: 0.5250\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.7161 - accuracy: 0.4750\n",
      "Test Accuracy: 0.4749999940395355\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: Load your data here\n",
    "\n",
    "# Example placeholder data - replace with your actual data\n",
    "X_train = np.random.randint(0, 10000, (1000, 100))  # Random data for illustration\n",
    "y_train = np.random.randint(0, 2, (1000, ))  # Random binary labels\n",
    "X_test = np.random.randint(0, 10000, (200, 100))  # Random data for illustration\n",
    "y_test = np.random.randint(0, 2, (200, ))  # Random binary labels\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),  # Increased complexity and added dropout\n",
    "    LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Using RMSprop optimizer and setting a learning rate\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a811d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('path_to_my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Data Frame Linux Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bcc5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the CSV into 4 parts with a prefix 'data_chunk_'\n",
    "# The -n l/4 option splits the file into 4 equal parts, by lines\n",
    "#This will create files named data_chunk_00, data_chunk_01,\n",
    "\n",
    "###   split -n l/10 -d your_large_file.csv data_chunk_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ebc1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmailClassification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Replace the path below with the path to your Hadoop-stored file\n",
    "df_emails = spark.read.csv(\"hdfs:///user1/data_batch_00\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d076f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "# Assuming the text column is named 'message'\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_emails)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Continue with further processing as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c4b78d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 10:09:40 ERROR Executor: Exception in task 1.0 in stage 30.0 (TID 51)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4435/158479843: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n",
      "\t... 18 more\n",
      "24/03/20 10:09:40 WARN TaskSetManager: Lost task 1.0 in stage 30.0 (TID 51) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4435/158479843: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n",
      "\t... 18 more\n",
      "\n",
      "24/03/20 10:09:40 ERROR TaskSetManager: Task 1 in stage 30.0 failed 1 times; aborting job\n",
      "24/03/20 10:09:40 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 50) (10.0.2.15 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o712.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 30.0 failed 1 times, most recent failure: Lost task 1.0 in stage 30.0 (TID 51) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4435/158479843: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4435/158479843: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4360/1272679267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# If everything is correct up to here, then converting to a Pandas DataFrame should work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpandas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o712.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 30.0 failed 1 times, most recent failure: Lost task 1.0 in stage 30.0 (TID 51) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4435/158479843: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (RegexTokenizer$$Lambda$4435/158479843: (string) => array<string>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer.$anonfun$createTransformFunc$2(Tokenizer.scala:146)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Assuming 'df_emails' is your initial DataFrame and 'message' is the column with email texts\n",
    "# Ensure 'df_emails' has been defined correctly and contains the 'message' column\n",
    "\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_emails)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Check if the 'filtered_words' column exists\n",
    "filtered_df.printSchema()\n",
    "\n",
    "# If everything is correct up to here, then converting to a Pandas DataFrame should work\n",
    "pandas_df = filtered_df.select(\"filtered_words\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04311039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Filter out rows where the message column is null\n",
    "df_non_null = df_emails.filter(col(\"message\").isNotNull())\n",
    "\n",
    "# Tokenize the email content\n",
    "tokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(df_non_null)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Continue with your processing...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21649907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "pandas_df = filtered_df.select(\"filtered_words\").toPandas()\n",
    "\n",
    "# Now, use Keras' text processing tools as shown previously to tokenize and pad sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "078cb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Placeholder for your data\n",
    "texts = [\"Your preprocessed email texts here...\"]  # This should be a list of email texts\n",
    "labels = [0, 1, 0, 1]  # Binary labels for each text\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)  # num_words is the size of your vocabulary\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)  # maxlen is the length of the longest sequence\n",
    "\n",
    "# Assuming labels are already prepared\n",
    "import numpy as np\n",
    "labels = np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7967ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1, 100)\n",
      "Labels shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b7eb419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (4, 100)\n",
      "Labels shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example placeholder, replace these with actual preprocessed data and labels\n",
    "texts = [\"sample text 1\", \"sample text 2\", \"sample text 3\", \"sample text 4\"]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)  # Ensure this matches your sequence length\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Now proceed with train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05a60a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 24)                408       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160433 (626.69 KB)\n",
      "Trainable params: 160433 (626.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 0.6947 - accuracy: 0.3333 - val_loss: 0.6914 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6937 - accuracy: 0.3333 - val_loss: 0.6940 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6929 - accuracy: 0.6667 - val_loss: 0.6963 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6921 - accuracy: 0.6667 - val_loss: 0.6986 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6914 - accuracy: 0.6667 - val_loss: 0.7010 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6906 - accuracy: 0.6667 - val_loss: 0.7033 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6898 - accuracy: 0.6667 - val_loss: 0.7055 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6891 - accuracy: 0.6667 - val_loss: 0.7073 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6885 - accuracy: 0.6667 - val_loss: 0.7092 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6879 - accuracy: 0.6667 - val_loss: 0.7111 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6873 - accuracy: 0.6667 - val_loss: 0.7131 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6867 - accuracy: 0.6667 - val_loss: 0.7150 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6861 - accuracy: 0.6667 - val_loss: 0.7170 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6855 - accuracy: 0.6667 - val_loss: 0.7188 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6850 - accuracy: 0.6667 - val_loss: 0.7207 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6844 - accuracy: 0.6667 - val_loss: 0.7226 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6838 - accuracy: 0.6667 - val_loss: 0.7245 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6833 - accuracy: 0.6667 - val_loss: 0.7263 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6827 - accuracy: 0.6667 - val_loss: 0.7282 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6821 - accuracy: 0.6667 - val_loss: 0.7302 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6816 - accuracy: 0.6667 - val_loss: 0.7322 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6810 - accuracy: 0.6667 - val_loss: 0.7341 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6804 - accuracy: 0.6667 - val_loss: 0.7362 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6799 - accuracy: 0.6667 - val_loss: 0.7382 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6793 - accuracy: 0.6667 - val_loss: 0.7403 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6787 - accuracy: 0.6667 - val_loss: 0.7424 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6781 - accuracy: 0.6667 - val_loss: 0.7445 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6776 - accuracy: 0.6667 - val_loss: 0.7467 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6770 - accuracy: 0.6667 - val_loss: 0.7489 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6764 - accuracy: 0.6667 - val_loss: 0.7512 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7512 - accuracy: 0.0000e+00\n",
      "Validation Loss: 0.7511619329452515\n",
      "Validation Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=16, input_length=100),  # input_dim is the size of the vocabulary, output_dim is the dimension of the dense embedding\n",
    "    GlobalAveragePooling1D(),  # This will average the embeddings of all words in the sequence\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification (0 or 1 labels)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3308280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6909 - accuracy: 0.6667 - val_loss: 0.7179 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6823 - accuracy: 0.6667 - val_loss: 0.7376 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.6748 - accuracy: 0.6667 - val_loss: 0.7595 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6666 - accuracy: 0.6667 - val_loss: 0.7815 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.6584 - accuracy: 0.6667 - val_loss: 0.8083 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6504 - accuracy: 0.6667 - val_loss: 0.8437 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.6416 - accuracy: 0.6667 - val_loss: 0.8907 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.6322 - accuracy: 0.6667 - val_loss: 0.9536 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.6232 - accuracy: 0.6667 - val_loss: 1.0363 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.6158 - accuracy: 0.6667 - val_loss: 1.1320 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.6123 - accuracy: 0.6667 - val_loss: 1.2247 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.6128 - accuracy: 0.6667 - val_loss: 1.2556 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6116 - accuracy: 0.6667 - val_loss: 1.2354 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6064 - accuracy: 0.6667 - val_loss: 1.1898 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.5998 - accuracy: 0.6667 - val_loss: 1.1387 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5935 - accuracy: 0.6667 - val_loss: 1.0924 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.5882 - accuracy: 0.6667 - val_loss: 1.0558 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.5835 - accuracy: 0.6667 - val_loss: 1.0298 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5787 - accuracy: 0.6667 - val_loss: 1.0142 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.5733 - accuracy: 0.6667 - val_loss: 1.0080 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.5672 - accuracy: 0.6667 - val_loss: 1.0101 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5600 - accuracy: 0.6667 - val_loss: 1.0195 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.5520 - accuracy: 0.6667 - val_loss: 1.0354 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.5431 - accuracy: 0.6667 - val_loss: 1.0565 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5335 - accuracy: 0.6667 - val_loss: 1.0812 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.5234 - accuracy: 0.6667 - val_loss: 1.1071 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5127 - accuracy: 0.6667 - val_loss: 1.1307 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5013 - accuracy: 0.6667 - val_loss: 1.1479 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4889 - accuracy: 0.6667 - val_loss: 1.1555 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4753 - accuracy: 0.6667 - val_loss: 1.1516 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=100, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3772b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6777 - accuracy: 0.6667 - val_loss: 0.7237 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.6848 - accuracy: 0.6667 - val_loss: 0.7403 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.6712 - accuracy: 0.6667 - val_loss: 0.7614 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.7053 - accuracy: 0.6667 - val_loss: 0.7783 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.6610 - accuracy: 0.6667 - val_loss: 0.7972 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6236 - accuracy: 0.6667 - val_loss: 0.8238 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3aecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359613e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18aa222a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83014665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b80f5cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 09:26:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f8aed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0f74a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "Number of rows: 2\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|      filtered_words|        raw_features|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Another email con...|[another, email, ...|[another, email, ...|(6,[0,1,3,4],[1.0...|(6,[0,1,3,4],[0.0...|\n",
      "|Email content exa...|[email, content, ...|[email, content, ...|(6,[0,1,2,5],[1.0...|(6,[0,1,2,5],[0.0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f786ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                text|\n",
      "+-------+--------------------+\n",
      "|  count|                   2|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|Another email con...|\n",
      "|    max|Email content exa...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dcdc174",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `message` cannot be resolved. Did you mean one of the following? [`text`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4360/3126844713.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_message'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-z ]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \"\"\"\n\u001b[1;32m   2927\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2928\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2929\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2930\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `message` cannot be resolved. Did you mean one of the following? [`text`]."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bce61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdda03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a53c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffcdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795585fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad373248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3c67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9493ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37106002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abbcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59fc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54143c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9812fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd12ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dc0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f1b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004be08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50146c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b25a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08912385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7998bbf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ff6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9930f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b26313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f58620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8629c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75c970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17e8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66ebcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c3109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b61bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee443af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32528d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26331c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96545a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5949c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0dfa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa3927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced810d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a91b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd28b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab87719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a04e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b4e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b2cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02541bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2b0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16c620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedf085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca78655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e3c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d886b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c47354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f3743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f669ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac73272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bf171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe21f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ebf7592",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'email_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4360/53892679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Example usage with matching lengths for email_texts and predicted_probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mvisualize_fraud_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'email_texts' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_fraud_predictions(email_texts, predictions):\n",
    "    \"\"\"\n",
    "    Visualize the emails with their predicted fraud probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - email_texts: List of email text content.\n",
    "    - predictions: List of predicted probabilities corresponding to the fraud likelihood of each email.\n",
    "\n",
    "    The function doesn't return anything but prints each email with its fraud prediction.\n",
    "    \"\"\"\n",
    "    # Ensure the lists have the same length\n",
    "    if len(email_texts) != len(predictions):\n",
    "        print(\"Error: The length of email_texts and predictions must match.\")\n",
    "        return\n",
    "    \n",
    "    for email, probability in zip(email_texts, predictions):\n",
    "        print(\"Email Content:\\n\", email)\n",
    "        # Assuming the probability is given in a scale from 0 to 1\n",
    "        print(\"Fraud Likelihood: {:.2%}\".format(probability))\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# Assuming the predictions list matches the number of email_texts provided\n",
    "predicted_probabilities = [0.34725702, 0.08264993]\n",
    "\n",
    "# Example usage with matching lengths for email_texts and predicted_probabilities\n",
    "visualize_fraud_predictions(email_texts, predicted_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a3b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f68a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emails_by_similarity_and_likelihood(emails, similarity_threshold=0.5, likelihood_threshold=8.0):\n",
    "    \"\"\"\n",
    "    Filters emails based on content similarity to a given phrase and a likelihood threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - emails: List of dictionaries, where each dictionary contains 'content' and 'likelihood' keys.\n",
    "    - similarity_threshold: A threshold for determining content similarity (not used in this simple example).\n",
    "    - likelihood_threshold: The minimum likelihood score for an email to be considered suspicious.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of emails considered suspicious based on the likelihood threshold.\n",
    "    \"\"\"\n",
    "    suspicious_phrase = \"Another suspicious email detected!\"\n",
    "    filtered_emails = [email for email in emails if suspicious_phrase in email['content'] and email['likelihood'] >= likelihood_threshold]\n",
    "    return filtered_emails\n",
    "\n",
    "# Example usage:\n",
    "emails = [\n",
    "    {'content': \"This is a normal email content.\", 'likelihood': 2.0},\n",
    "    {'content': \"Another suspicious email detected! Please check it out.\", 'likelihood': 8.26},\n",
    "    {'content': \"Another suspicious email detected! This seems like a scam.\", 'likelihood': 9.5},\n",
    "    {'content': \"This is another normal conversation.\", 'likelihood': 3.2}\n",
    "]\n",
    "\n",
    "# Filtering emails:\n",
    "suspicious_emails = filter_emails_by_similarity_and_likelihood(emails, likelihood_threshold=8.0)\n",
    "\n",
    "# Displaying the filtered, suspicious emails:\n",
    "for email in suspicious_emails:\n",
    "    print(f\"Email Content: {email['content']}\")\n",
    "    print(f\"Fraud Likelihood: {email['likelihood']}%\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92e1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73ec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd9ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f113b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa12248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74514c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a04849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f426539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc31de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26079f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5844a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde65d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169719d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85238ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df98e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
